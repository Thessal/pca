{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922a3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prc = pd.read_csv(\"./close.csv\", index_col=0, parse_dates=True)\n",
    "df_logret = df_prc.apply(np.log).diff().iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde83714",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_logret = torch.Tensor(df_logret.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126433",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 5\n",
    "ts = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=lookback, padding_mode='zeros', padding=lookback-1, bias=False)\n",
    "ts.weight.data = ts.weight*0 + 1\n",
    "# ts_operator\n",
    "x_logret_ts = ts(x_logret.T[:,torch.newaxis,:]).transpose(0,1)[0,:,:-lookback+1].T#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_logret\n",
    "cs = nn.Linear(in_features=x.shape[1], out_features=x.shape[1], bias=False)\n",
    "cs.weight.data = cs.weight*0 + 1\n",
    "# cs operator\n",
    "x_logret_cs = cs(x_logret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b856e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without batch\n",
    "lookback = 5\n",
    "x = x_logret\n",
    "\n",
    "ts_layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=lookback, padding_mode='zeros', padding=lookback-1, bias=False)\n",
    "cs_layer = nn.Linear(in_features=x.shape[1], out_features=x.shape[1], bias=False)\n",
    "ts = lambda x: ts_layer(x.T[:,torch.newaxis,:]).transpose(0,1)[0,:,:-lookback+1].T\n",
    "cs = lambda x: cs_layer(x)\n",
    "csts_output = ts(cs(x))\n",
    "csts_output = (csts_output - csts_output.mean(axis=1, keepdim=True))\n",
    "csts_output = csts_output / csts_output.abs().sum(axis=1, keepdim=True) # long 0.5 short 0.5 \n",
    "alpha = csts_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e34bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size = n_alpha\n",
    "lookback = 5\n",
    "n_alpha = 30\n",
    "x = x_logret\n",
    "\n",
    "ts_layer = nn.Conv1d(in_channels=n_alpha, out_channels=n_alpha, groups=n_alpha, kernel_size=lookback, padding_mode='zeros', padding=lookback-1, bias=False)\n",
    "cs_layer = nn.Linear(in_features=x.shape[1], out_features=x.shape[1], bias=False) # depthwise\n",
    "ts = lambda x: ts_layer(x.permute(2,0,1)).permute(1,2,0)[:,:-lookback+1,:]\n",
    "cs = lambda x: cs_layer(x)\n",
    "x_repeat = x.repeat(n_alpha,1,1) # batch, time, instrument\n",
    "csts_output = ts(cs(x_repeat))  \n",
    "csts_output = (csts_output - csts_output.mean(axis=2, keepdim=True))\n",
    "csts_output = csts_output / csts_output.abs().sum(axis=2, keepdim=True) # long 0.5 short 0.5 \n",
    "alpha_returns = csts_output * x_repeat\n",
    "alpha_returns = alpha_returns.sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_cov = alpha_returns.cov()\n",
    "alpha_eigval, alpha_eigvec = torch.linalg.eigh(alpha_cov)\n",
    "alpha_weights = sum([a*v for a,v in zip(torch.sqrt(alpha_eigval), alpha_eigvec.T)])\n",
    "alpha_weights = alpha_weights / (alpha_weights.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weights = (csts_output * (alpha_weights[:,torch.newaxis,torch.newaxis])).sum(axis=0)\n",
    "returns = (alpha_returns * alpha_weights[:,np.newaxis]).sum(axis=0)\n",
    "# returns_val = (raw_weights * x_logret).sum(axis=1)\n",
    "# assert ( returns_val - returns ).abs().max() < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9735cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = returns.mean() / returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e362b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97b632f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0032, -0.0008, -0.0003,  ...,  0.0035, -0.0021, -0.0019],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = alpha * x\n",
    "ret.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval, evec = torch.linalg.eigh(x_tf.T.cov()) # eval[i] , evec[:,i]\n",
    "# x_tf.T.cov() - sum([a*(v[:,torch.newaxis]@v[torch.newaxis,:]) for a,v in zip(eval,evec.T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factor = 30\n",
    "weights = sum([v/torch.sqrt(a) for a,v in zip(eval[-n_factor:],evec.T[-n_factor:])])\n",
    "normalize = weights.abs().sum() * 0.5 # long 1 short 1\n",
    "weights = weights / normalize \n",
    "variance = n_factor / normalize # weights @ (x_tf.T.cov()) @ weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a85cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2009e-04,  3.8787e-04,  1.3604e-03, -1.4446e-02,  3.0492e-03,\n",
       "         6.7464e-04,  7.7460e-03,  2.1189e-03, -9.9293e-03,  1.6725e-03,\n",
       "        -9.1427e-03, -1.7301e-02, -7.5083e-03, -5.1239e-03,  1.3681e-02,\n",
       "        -1.7822e-02,  2.2641e-03,  1.1435e-02, -9.4208e-03, -1.3212e-02,\n",
       "        -1.3921e-02,  9.1543e-03, -9.4438e-03,  1.2114e-02,  2.2472e-02,\n",
       "         7.3175e-03,  3.5766e-03,  1.0507e-02,  8.2215e-03,  1.1521e-03,\n",
       "        -1.3140e-02,  2.6374e-03,  8.3662e-03, -1.1318e-02, -3.7795e-03,\n",
       "        -5.0300e-03, -5.4616e-04, -1.3189e-02,  1.4785e-02, -4.6221e-03,\n",
       "        -2.1622e-02,  1.7472e-02, -2.0424e-02, -3.5132e-02,  1.3000e-03,\n",
       "         6.2976e-03, -1.9479e-02, -5.5162e-03, -5.8511e-03,  1.6059e-02,\n",
       "        -1.2325e-02,  7.7561e-04,  1.1482e-02, -9.1925e-03, -4.3449e-03,\n",
       "         8.8252e-03,  1.9328e-02, -1.3388e-02,  1.0383e-02,  1.7444e-02,\n",
       "         2.4698e-03, -8.1510e-04,  1.3505e-02,  1.6301e-02, -7.1803e-03,\n",
       "         3.0053e-04, -1.8014e-02, -8.5565e-03,  3.3346e-03, -5.7330e-03,\n",
       "         2.2022e-02, -5.1161e-03, -1.9962e-02,  5.1597e-03,  4.7624e-03,\n",
       "         1.0584e-04, -1.7843e-02, -5.5432e-04,  2.3569e-03, -6.3547e-04,\n",
       "         1.5823e-02, -1.5264e-02,  2.6296e-03, -1.8533e-03,  1.4483e-02,\n",
       "         6.2277e-03, -8.3856e-03, -5.0704e-03,  3.0755e-03,  1.3717e-02,\n",
       "         6.4944e-03, -1.3367e-02,  7.4281e-03, -1.4133e-03,  6.6540e-03,\n",
       "        -5.0836e-03,  2.3601e-03,  1.2576e-02,  4.2000e-03,  1.1071e-02,\n",
       "        -1.3530e-02,  4.6092e-03, -2.4294e-04, -5.8567e-03,  1.1198e-02,\n",
       "         8.3665e-03, -2.6200e-03,  9.7099e-03,  2.2017e-02,  9.8013e-03,\n",
       "        -4.6023e-03, -1.6585e-02, -7.5272e-03, -2.2215e-03, -2.4682e-03,\n",
       "         1.2005e-02,  4.1349e-03, -5.0059e-03,  4.6594e-03,  1.7268e-02,\n",
       "        -1.1044e-02,  1.6351e-02,  5.8581e-06, -1.0046e-02, -9.8379e-04,\n",
       "         8.5718e-03, -1.6723e-02,  1.5073e-03,  3.3857e-04, -6.7314e-03,\n",
       "         2.7788e-02,  5.0817e-04,  1.9299e-02,  2.3458e-02,  2.0106e-02,\n",
       "        -5.3659e-03, -1.2479e-03,  4.1681e-03, -6.0285e-03,  1.0858e-02,\n",
       "        -3.7443e-03, -4.2334e-03,  2.1285e-03,  5.3483e-03, -3.2613e-03,\n",
       "        -4.8682e-03, -1.6587e-02, -1.5914e-02, -1.6496e-02, -2.2545e-02,\n",
       "        -1.7944e-03, -1.3016e-02,  1.3727e-02,  1.6646e-02,  2.2329e-04,\n",
       "         1.1640e-02,  1.0936e-02, -1.8877e-02,  8.7553e-03, -1.2872e-02,\n",
       "         3.0000e-03, -6.8829e-03,  1.8382e-04, -9.8165e-03, -2.1242e-02,\n",
       "        -3.3411e-03,  8.9027e-03,  4.7356e-03,  1.3707e-02,  8.4867e-03,\n",
       "         1.6863e-03, -9.8407e-03,  1.0585e-02,  1.2603e-02,  1.8570e-02,\n",
       "         1.4304e-02,  4.1129e-03, -1.1250e-02, -1.2932e-02,  1.5318e-02,\n",
       "         3.7756e-04,  1.1291e-02, -3.1145e-03, -1.9312e-02,  2.9317e-03,\n",
       "        -1.5178e-02,  1.1227e-02,  6.6816e-03, -1.0226e-02,  2.4535e-03,\n",
       "        -1.0680e-02, -2.7390e-02,  1.2993e-02,  1.3799e-02,  1.5031e-03,\n",
       "         6.7300e-03, -2.5094e-02,  2.2375e-02,  1.7147e-02, -1.1085e-02,\n",
       "         2.2289e-02, -2.9264e-03,  1.0894e-02,  2.4812e-02, -7.4266e-03,\n",
       "         4.6227e-03, -8.1624e-03, -1.2911e-02, -7.8291e-03, -5.0480e-03,\n",
       "         4.1685e-03, -7.3971e-03, -2.8218e-03, -4.0783e-03],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65516058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.5498e-06, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# torch.bilinear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1759cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MinVar(nn.Module):\n",
    "    def __init__(self, cov, gross_exposure_constraint, use_latent=False, cuda=True):\n",
    "        super().__init__()\n",
    "        self.enable_cuda = cuda\n",
    "        self.cov = cov.cuda() if cuda else cov\n",
    "        num_instrument = cov.shape[0]\n",
    "        self.use_latent = use_latent\n",
    "        if use_latent:\n",
    "            self.latent = nn.Linear(in_features=30, out_features=num_instrument, bias=False) # A\n",
    "            self.fc = nn.Linear(in_features=30, out_features=1, bias=False) # w_e\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=num_instrument, out_features=1, bias=False) # w_e\n",
    "        self.fc.weight.data = (torch.rand_like(self.fc.weight) - 0.5) / num_instrument \n",
    "        self.gross_exp_constr = gross_exposure_constraint\n",
    "\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(self.cov)\n",
    "        sorted_indices = torch.argsort(eigenvalues, descending=True)\n",
    "        self.eigenvalues = eigenvalues[sorted_indices]\n",
    "        self.eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    @property\n",
    "    def portfolio_weight(self): # w_p = v * A * w_e\n",
    "        if self.use_latent:\n",
    "            weight_in_eigenvalue_space = self.latent(self.fc.weight).T\n",
    "        weight_in_eigenvalue_space = self.fc.weight.T\n",
    "        weight_in_instrument_space = self.eigenvectors @ weight_in_eigenvalue_space\n",
    "        norm_coef = self.normalize_coeff(weight_in_instrument_space)\n",
    "        return weight_in_eigenvalue_space / norm_coef, weight_in_instrument_space / norm_coef\n",
    "    \n",
    "    def get_weight(self):\n",
    "        return self.portfolio_weight[1].cpu().detach().numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_coeff(x):\n",
    "        # 1x Long\n",
    "        return x.sum()\n",
    "    \n",
    "    def _normalize_weight(self):\n",
    "        # Prevents divergence\n",
    "        if self.use_latent:\n",
    "            self.latent.weight.data = self.latent.weight / torch.linalg.matrix_norm(self.latent.weight, ord='fro')\n",
    "        # self.fc.weight.data = self.fc.weight / torch.linalg.vector_norm(self.fc.weight, ord=2)\n",
    "        self.fc.weight.data = self.fc.weight / self.normalize_coeff(self.fc.weight)\n",
    "\n",
    "    def fit(self, n_epochs=10000, print_step=1000, learning_rate=1e-4):\n",
    "        if self.enable_cuda:\n",
    "            self.cuda()\n",
    "        optimizer = torch.optim.Adagrad(self.parameters(), lr=learning_rate) # Momentum-less optimizer for barrier method\n",
    "        for e in range(n_epochs):\n",
    "            w_e, w_i = self.portfolio_weight\n",
    "            gross_exposure = torch.linalg.vector_norm(w_i, ord=1)\n",
    "            portfolio_variance = self.eigenvalues @ (w_e.square())\n",
    "            barrier_loss = torch.relu(gross_exposure - self.gross_exp_constr)\n",
    "            loss = torch.log(portfolio_variance) + barrier_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # self._normalize_weight()\n",
    "            print(f'{str(e).zfill(5)}/{n_epochs}, variance={portfolio_variance.item():.2e}, gross_exposure = {gross_exposure.item():.2e}, barrier_loss={barrier_loss:.2e}, loss={loss.item():.2e}, normalization={torch.sum(self.fc.weight):.2e}     ', end='\\r', flush=True)\n",
    "            if print_step and e%print_step == 0 :\n",
    "                print()\n",
    "        print(\"\\nTrain Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d48158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
